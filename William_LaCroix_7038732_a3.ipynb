{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mg:\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('g:')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93m/g:/My Drive/UdS/Classes/Computational Linguistics/a3/Submission/atis-grammar-cnf.cfg\u001b[0m\n\n  Searched in:\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m Tree \u001b[39mas\u001b[39;00m Tree\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m grammar \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mfile:///\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(\u001b[39m'\u001b[39;49m\u001b[39matis-grammar-cnf.cfg\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39m# load the grammar\u001b[39;00m\n\u001b[0;32m      7\u001b[0m s \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mfile:///\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m'\u001b[39m\u001b[39matis-test-sentences.txt\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m# load raw sentences\u001b[39;00m\n\u001b[0;32m      8\u001b[0m t \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mextract_test_sentences(s) \u001b[39m# extract test sentences \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:879\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m--> 879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    880\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m urlopen(resource_url)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mg:\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('g:')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93m/g:/My Drive/UdS/Classes/Computational Linguistics/a3/Submission/atis-grammar-cnf.cfg\u001b[0m\n\n  Searched in:\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import CFG as CFG\n",
    "from nltk.tree import Tree as Tree\n",
    "import numpy as np\n",
    "\n",
    "grammar = nltk.data.load('file:///' + os.path.abspath('atis-grammar-cnf.cfg')) # load the grammar\n",
    "s = nltk.data.load('file:///' + os.path.abspath('atis-test-sentences.txt')) # load raw sentences\n",
    "t = nltk.parse.util.extract_test_sentences(s) # extract test sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuffRule:\n",
    "    def __init__(self, A, B, Bij=None, C=None, Cij=None): # 'None' arguments let this work for leaves as well as nodes\n",
    "        \"\"\"\n",
    "        Back pointers are confusing. Don't let them get you down!\n",
    "        \n",
    "        ex: production rule: HMC -> NOUN_NN HMD\n",
    "        becomes: production() = BuffRule(   A=str(\"HMC\"),\n",
    "                                            B=str(\"NOUN_NN\"),\n",
    "                                            Bij=tuple(DataFram[i], DataFram[j]),\n",
    "                                            C=str(\"HMD\"),\n",
    "                                            Cij=tuple(DataFram[i], DataFram[j]))\n",
    "\n",
    "        So, when building your trees, it's as simple as looking up production rule A, looking into Bij and Cij, and checking B/C.\n",
    "        \"\"\"\n",
    "        self.A = A  # Rule LHS\n",
    "        self.B = B  # Rule RHS1\n",
    "        self.Bij = Bij # Rule RHS1 index\n",
    "        self.C = C # Rule RHS2\n",
    "        self.Cij = Cij # Rule RHS2 index\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A # for checking the \"name\" of this rule in a cell, all we need is the LHS A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data structure: Ch(i,k) eventually contains {A | A ⇒* wi ... wk-1} (initially empty)\n",
    "\n",
    "for each i from 1 to n:\n",
    "    for each production rule A → wi:\n",
    "        add A to Ch(i, i+1) \n",
    "\n",
    "for each width b from 2 to n:\n",
    "    for each start position i from 1 to n-b+1:\n",
    "        for each left width k from 1 to b-1:\n",
    "            for each B ∈ Ch(i, i+k) and C ∈ Ch(i+k,i+b):\n",
    "                for each production rule A → B C:\n",
    "                    add A to Ch(i,i+b) claim that w ∈ L(G) iff S ∈ Ch(1,n+1)\n",
    "\"\"\"\n",
    "\n",
    "def parse(sentence):\n",
    "    if type(sentence) == str:\n",
    "        sentence = sentence.split()\n",
    "    n = len(sentence) # n | n = word length\n",
    "    Ch = np.empty((n,n), dtype=set) # nxn array of sets, array[row[list[parent node(s)]]]\n",
    "    buffCh = np.empty((n,n), dtype=set) # nxn array of sets of nodes(BuffRule objects), array[row[list[BuffRules(s)]]]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Ch[i][j] = set() # there must be a way to populate with empty sets, I just haven't found it.\n",
    "            buffCh[i][j] = set()\n",
    "    for j in range(n): # for each i from 1 to n:\n",
    "        for rule in grammar.productions(rhs=sentence[j]):\n",
    "            Ch[j][j].add(rule.lhs()) # for each production rule A → wi: add A to Ch(i, i+1)\n",
    "            buffCh[j][j].add(BuffRule(A=rule.lhs(), B=sentence[j]))\n",
    "    for j in range(n):\n",
    "        for i in range(j-1, -1, -1): # I hate these stepwise increments, especially since Koller does it reversed form the book,\n",
    "            for k in range(i, j):    #  and neither worked in practice, so I had to make my own loop index rules.\n",
    "                for rule in grammar.productions():\n",
    "                    if rule.rhs()[0] in Ch[i][k] and rule.rhs()[1] in Ch[k+1][j]:\n",
    "                        Ch[i][j].add(rule.lhs()) # rule grid for grammaticality check\n",
    "                        buffCh[i][j].add(BuffRule(A=rule.lhs(), B=rule.rhs()[0], Bij=(i,k), C=rule.rhs()[1], Cij=(k+1,j))) # BuffRule grid for building trees.\n",
    "    return buffCh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tree(node):\n",
    "    if node.C == None:\n",
    "        return f\"({node.A}: {node.B})\" # base case, print the leaf and its production rule\n",
    "    else:\n",
    "        for item in grid[node.Bij[0]][node.Bij[1]]: # checking rules in the cell indicated by backpointer for B\n",
    "            if item.A == node.B:\n",
    "                new_node_B = item\n",
    "        for item in grid[node.Cij[0]][node.Cij[1]]: # checking rules in the cell indicated by backpointer for C\n",
    "            if item.A == node.C:\n",
    "                new_node_C = item\n",
    "        return f\"({node.A} ({draw_tree(new_node_B)}  {draw_tree(new_node_C)}))\" # recursively building next branch/nodes in tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and drawing ATIS test sentences...\n",
    "\n",
    "    Used to take about 12s to run the recogniser, but now looking at counts it's suddenly taking several minutes?\n",
    "    Most recent run was 7m41s\n",
    "    Sorry :-\\\n",
    "    Refactored as much as I could, but coulnd't recreate the 12s checks. Dunno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "#trees = []\n",
    "# i = 0\n",
    "for sentence in t:\n",
    "    grid = parse(sentence[0]) #  >:(\n",
    "    count = 0\n",
    "    for item in grid[0][-1]: \n",
    "        if str(item.A) == 'SIGMA': # claim that w ∈ L(G) iff S ∈ Ch(1,n+1)\n",
    "            # trees.append(draw_tree(item)) # glorious, wonderful trees!\n",
    "            count += 1\n",
    "    results.append((sentence[0], count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twee = Tree.fromstring(trees[0])      ### Turn this section on for printing tree samples\n",
    "# print(twee.leaves())\n",
    "# twee_weaves = twee.leaves()\n",
    "# print(\" \".join(twee_weaves))\n",
    "# for tree in trees:\n",
    "#     print(Tree.fromstring(tree))\n",
    "\n",
    "with open(\"output.txt\", 'w', encoding=\"utf-8\") as file_out:\n",
    "    for result in results:\n",
    "        file_out.write(str(str(\" \".join(result[0])) + \"\\t\" + str(result[1]) + \" parses\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30c802e65b44523fa6e12230e816e2b6fa20dbc368188253f9eb5f32dc2eeffd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
